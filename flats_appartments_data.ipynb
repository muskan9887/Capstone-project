{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41384d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32bcdbf",
   "metadata": {},
   "source": [
    "**Overview**<br><br>\n",
    "This code sets up the necessary tools and headers to scrape web data. It imports libraries for sending HTTP requests, parsing HTML, and handling data. Additionally, it defines a user agent header to mimic a web browser, which can be useful to avoid getting blocked by some websites.\n",
    "\n",
    "\n",
    "\n",
    "- **import requests**\n",
    "\n",
    "    This line imports the requests module, which is a popular Python module used to send HTTP requests to websites.\n",
    "\n",
    "- **from bs4 import BeautifulSoup**\n",
    "    This line imports BeautifulSoup from the bs4 module. BeautifulSoup is a library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree that can be used to extract data in a hierarchical and more readable manner.\n",
    "    \n",
    "- **import os**\n",
    "\n",
    "    This line imports the os module, which provides a way of interacting with the operating system. This could be used for tasks like creating directories, reading environment variables, etc.\n",
    "    \n",
    "- **headers = {...}**\n",
    "\n",
    "    This line defines a dictionary called headers with a 'User-Agent' key. The value of this key is a string that represents a user agent string.\n",
    "\n",
    "    The user agent string is used to tell the server about the browser and operating system of the user. Some websites serve different content based on the user agent or even block certain user agents (often to prevent scraping). By defining a common browser's user agent string, this code is trying to mimic a real browser request to potentially avoid blocks or get the same content a real user would see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972f5ef",
   "metadata": {},
   "source": [
    "- Your_Project_Directory\n",
    "  - Data\n",
    "    - City\n",
    "      - Flats\n",
    "      - Societies\n",
    "      - Residential\n",
    "      - Independent House\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1450828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "City = 'chandigarh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fd4db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Agent\n",
    "# Headers set like below:\n",
    "# User Agent\n",
    "headers = {\n",
    "    'authority': 'www.99acres.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'no-cache',\n",
    "    'dnt': '1',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': f'https://www.99acres.com/flats-in-{City}-ffid-page',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"107\", \"Not;A=Brand\";v=\"8\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/527.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f446224d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: C:\\Users\\KIIT\\Desktop\\Capstone_project\\Data\n",
      "Created directory: C:\\Users\\KIIT\\Desktop\\Capstone_project\\Data/chandigarh\n",
      "Created directory: C:\\Users\\KIIT\\Desktop\\Capstone_project\\Data/chandigarh/Flats\n",
      "Created directory: C:\\Users\\KIIT\\Desktop\\Capstone_project\\Data/chandigarh/Societies\n",
      "Created directory: C:\\Users\\KIIT\\Desktop\\Capstone_project\\Data/chandigarh/Residential\n",
      "Created directory: C:\\Users\\KIIT\\Desktop\\Capstone_project\\Data/chandigarh/Independent House\n"
     ]
    }
   ],
   "source": [
    "# If folder structures are in already created no need to run it.\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the path to your project directory\n",
    "project_dir = 'C:\\\\Users\\\\KIIT\\\\Desktop\\\\Capstone_project'\n",
    "\n",
    "# Define the subdirectories\n",
    "subdirectories = ['Data', f'Data/{City}', f'Data/{City}/Flats', f'Data/{City}/Societies', f'Data/{City}/Residential', f'Data/{City}/Independent House']\n",
    "\n",
    "# Create the directory structure\n",
    "for subdir in subdirectories:\n",
    "    dir_path = os.path.join(project_dir, subdir)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(f\"Created directory: {dir_path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {dir_path}\")\n",
    "\n",
    "# Now, your directory structure is created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6daf1df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter page number where you got error in last run.\n",
      "Enter page number to start from:2\n",
      "2 -> 23\n",
      "3 -> 26\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(3678 bytes read, 6562 more expected)', IncompleteRead(3678 bytes read, 6562 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:444\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:831\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 831\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    832\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m    833\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    834\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:775\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m amt \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left:\n\u001b[1;32m--> 775\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m-\u001b[39m amt\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:633\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(3678 bytes read, 6562 more expected)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[0;32m    625\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:816\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BodyNotHttplibCompatible(\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody should be http.client.HTTPResponse like. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt should have have an fp attribute which returns raw chunks.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m     )\n\u001b[1;32m--> 816\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_error_catcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Don't bother reading the body of a HEAD request.\u001b[39;49;00m\n\u001b[0;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_original_response\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_response_to_head\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_original_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:461\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;66;03m# This includes IncompleteRead.\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e, e)\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# unnecessarily.\u001b[39;00m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(3678 bytes read, 6562 more expected)', IncompleteRead(3678 bytes read, 6562 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Detail Page\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m dpageSoup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m req \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\requests\\models.py:818\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(3678 bytes read, 6562 more expected)', IncompleteRead(3678 bytes read, 6562 more expected))"
     ]
    }
   ],
   "source": [
    "# Put start page number and end page number.\n",
    "\n",
    "# Page number to start extraction data\n",
    "start = int(input(\"Enter page number where you got error in last run.\\nEnter page number to start from:\")) # Starting Page\n",
    "\n",
    "# End Page number- you can change is for start i am taking 10pages at a time,\n",
    "# as IPs are gettig block after some time\n",
    "end = start+10\n",
    "\n",
    "pageNumber = start\n",
    "req=0\n",
    "\n",
    "flats = pd.DataFrame()\n",
    "\n",
    "try :\n",
    "    while pageNumber < end:\n",
    "        i=1\n",
    "        url = f'https://www.99acres.com/flats-in-{City}-ffid-page-{pageNumber}'\n",
    "        page = requests.get(url, headers=headers)\n",
    "        pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "        req+=1\n",
    "        for soup in pageSoup.select_one('div[data-label=\"SEARCH\"]').select('section[data-hydration-on-demand=\"true\"]'):\n",
    "\n",
    "        # Extract property name and property sub-name\n",
    "            try:\n",
    "                property_name = soup.select_one('a.srpTuple__propertyName').text.strip()\n",
    "                # Extract link\n",
    "                link = soup.select_one('a.srpTuple__propertyName')['href']\n",
    "                society = soup.select_one('#srp_tuple_society_heading').text.strip()\n",
    "            except:\n",
    "                continue\n",
    "            # Detail Page\n",
    "            page = requests.get(link, headers=headers)\n",
    "            dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "            req += 1\n",
    "            try:\n",
    "                #price Range\n",
    "                price = dpageSoup.select_one('#pdPrice2').text.strip()\n",
    "            except:\n",
    "                price = ''\n",
    "\n",
    "            # Area\n",
    "            try:\n",
    "                area = soup.select_one('#srp_tuple_price_per_unit_area').text.strip()\n",
    "            except:\n",
    "                area =''\n",
    "            # Area with Type\n",
    "            try:\n",
    "                areaWithType = dpageSoup.select_one('#factArea').text.strip()\n",
    "            except:\n",
    "                areaWithType = ''\n",
    "\n",
    "\n",
    "            # Configuration\n",
    "            try:\n",
    "                bedRoom = dpageSoup.select_one('#bedRoomNum').text.strip()\n",
    "            except:\n",
    "                bedRoom = ''\n",
    "            try:\n",
    "                bathroom = dpageSoup.select_one('#bathroomNum').text.strip()\n",
    "            except:\n",
    "                bathroom = ''\n",
    "            try:\n",
    "                balcony = dpageSoup.select_one('#balconyNum').text.strip()\n",
    "            except:\n",
    "                balcony = ''\n",
    "\n",
    "            try:\n",
    "                additionalRoom = dpageSoup.select_one('#additionalRooms').text.strip()\n",
    "            except:\n",
    "                additionalRoom = ''\n",
    "\n",
    "\n",
    "            # Address\n",
    "\n",
    "            try:\n",
    "                address = dpageSoup.select_one('#address').text.strip()\n",
    "            except:\n",
    "                address = ''\n",
    "            # Floor Number\n",
    "            try:\n",
    "                floorNum = dpageSoup.select_one('#floorNumLabel').text.strip()\n",
    "            except:\n",
    "                floorNum = ''\n",
    "\n",
    "            try:\n",
    "                facing = dpageSoup.select_one('#facingLabel').text.strip()\n",
    "            except:\n",
    "                facing = ''\n",
    "\n",
    "            try:\n",
    "                agePossession = dpageSoup.select_one('#agePossessionLbl').text.strip()\n",
    "            except:\n",
    "                agePossession = ''\n",
    "\n",
    "            # Nearby Locations\n",
    "\n",
    "            try:\n",
    "                nearbyLocations = [i.text.strip() for i in dpageSoup.select_one('div.NearByLocation__tagWrap').select('span.NearByLocation__infoText')]\n",
    "            except:\n",
    "                nearbyLocations = ''\n",
    "\n",
    "            # Descriptions\n",
    "            try:\n",
    "                description = dpageSoup.select_one('#description').text.strip()\n",
    "            except:\n",
    "                description = ''\n",
    "\n",
    "            # Furnish Details\n",
    "            try:\n",
    "                furnishDetails = [i.text.strip() for i in dpageSoup.select_one('#FurnishDetails').select('li')]\n",
    "            except:\n",
    "                furnishDetails = ''\n",
    "\n",
    "            # Features\n",
    "            if furnishDetails:\n",
    "                try:\n",
    "                    features = [i.text.strip() for i in dpageSoup.select('#features')[1].select('li')]\n",
    "                except:\n",
    "                    features = ''\n",
    "            else:\n",
    "                try:\n",
    "                    features = [i.text.strip() for i in dpageSoup.select('#features')[0].select('li')]\n",
    "                except:\n",
    "                    features = ''\n",
    "\n",
    "\n",
    "\n",
    "            # Rating by Features\n",
    "            try:\n",
    "                rating = [i.text for i in dpageSoup.select_one('div.review__rightSide>div>ul>li>div').select('div.ratingByFeature__circleWrap')]\n",
    "            except:\n",
    "                rating = ''\n",
    "            # print(top_f)\n",
    "\n",
    "            try:\n",
    "                # Property ID\n",
    "                property_id = dpageSoup.select_one('#Prop_Id').text.strip()\n",
    "            except:\n",
    "                property_id = ''\n",
    "\n",
    "            # Create a dictionary with the given variables\n",
    "            property_data = {\n",
    "            'property_name': property_name,\n",
    "            'link': link,\n",
    "            'society': society,\n",
    "            'price': price,\n",
    "            'area': area,\n",
    "            'areaWithType': areaWithType,\n",
    "            'bedRoom': bedRoom,\n",
    "            'bathroom': bathroom,\n",
    "            'balcony': balcony,\n",
    "            'additionalRoom': additionalRoom,\n",
    "            'address': address,\n",
    "            'floorNum': floorNum,\n",
    "            'facing': facing,\n",
    "            'agePossession': agePossession,\n",
    "            'nearbyLocations': nearbyLocations,\n",
    "            'description': description,\n",
    "            'furnishDetails': furnishDetails,\n",
    "            'features': features,\n",
    "            'rating': rating,\n",
    "            'property_id': property_id\n",
    "        }\n",
    "\n",
    "\n",
    "            temp_df = pd.DataFrame.from_records([property_data])\n",
    "            # print(temp_df)\n",
    "            flats = pd.concat([flats, temp_df], ignore_index=True)\n",
    "            i += 1\n",
    "            # if os.path.isfile(csv_file):\n",
    "            # # Append DataFrame to the existing file without header\n",
    "            #     temp_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "            # else:\n",
    "            #     # Write DataFrame to the file with header\n",
    "            #     temp_df.to_csv(csv_file, mode='a', header=True, index=False)\n",
    "\n",
    "            if req % 4==0:\n",
    "                time.sleep(10)\n",
    "            if req % 15 == 0:\n",
    "                time.sleep(50)\n",
    "        print(f'{pageNumber} -> {i}')\n",
    "        pageNumber += 1\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(e)\n",
    "    print(\"----------------\")\n",
    "    print(\"\"\"Your IP might have blocked. Delete Runitme and reconnect again with updating start page number.\\n\n",
    "            You would see in output above like 1 -> 15\\ and so 1 is page number and 15 is data items extracted.\"\"\")\n",
    "    csv_file_path = f\"C:\\\\Users\\\\KIIT\\\\Desktop\\\\Capstone_project\\\\Data\\\\chandigarh\\\\Flats\\\\flats_{City}_data-page-{start}-{pageNumber-1}.csv\"\n",
    "\n",
    "    # This file will be new every time if start page will chnage, but still taking here mode as append\n",
    "    if os.path.isfile(csv_file_path):\n",
    "    # Append DataFrame to the existing file without header\n",
    "        flats.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write DataFrame to the file with header - first time write\n",
    "        flats.to_csv(csv_file_path, mode='a', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e3932",
   "metadata": {},
   "source": [
    "**Overview**<br><br>\n",
    "The code scrapes property data from the website \"99acres.com\" for apartments in Gurgaon. It navigates through a range of pages, extracts details of each property, and saves the data to a CSV file. The script is designed to handle potential errors gracefully, using try and except blocks to manage missing data, and introduces pauses to avoid making rapid requests and potentially getting blocked by the website.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Initialization of Variables**:\n",
    "\n",
    "    - start and end specify the range of web pages to scrape.\n",
    "    - csv_file defines the path to the CSV file where data will be saved.\n",
    "    - pageNumber starts from the initial value of start and will be incremented to navigate through the pages.\n",
    "    - req counts the number of HTTP requests made.\n",
    "\n",
    "- **Loop for Page Navigation**:\n",
    "\n",
    "    - The while loop is used to navigate through each page in the range from start to end.\n",
    "    - Inside this loop, the URL of the page to be scraped is constructed using the pageNumber.\n",
    "    - An HTTP GET request is made to retrieve the content of the page, and the content is then parsed using BeautifulSoup.\n",
    "\n",
    "- **Loop for Property Extraction**:\n",
    "\n",
    "    - The nested for loop navigates through individual property sections on the current page.\n",
    "    - The script attempts to extract the property name, its link, and its society name.\n",
    "    - If any of these attributes are missing, it skips to the next property.\n",
    "\n",
    "- **Detail Extraction**:\n",
    "\n",
    "    - For each property, an HTTP request is made to its detail page.\n",
    "    - The code then attempts to extract various property details like price, area, bedroom count, bathroom count, balcony count, address, and many other attributes. If any attribute is missing, the code handles it gracefully, assigning an empty string or an empty list as appropriate.\n",
    "\n",
    "- **Creating and Saving Data**:\n",
    "\n",
    "    - All extracted details are stored in a dictionary named property_data.\n",
    "    - This dictionary is then converted to a temporary DataFrame temp_df.\n",
    "    - The data is appended to a main DataFrame flats and also saved to the CSV file. If the file already exists, the new data is appended without writing the headers again.\n",
    "\n",
    "- **Request Management**:\n",
    "\n",
    "    - To avoid making too many rapid requests (which can lead to IP bans), the script introduces pauses.\n",
    "    - Every 4 requests, it pauses for 10 seconds. Every 15 requests, it pauses for 50 seconds.\n",
    "\n",
    "- **Page Counter and Loop Increment**:\n",
    "\n",
    "    - After scraping all properties on a page, the code prints the page number and the number of properties processed.\n",
    "    - pageNumber is incremented to move to the next page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine multiple csv file is one file.\n",
    "\n",
    "def combine_csv_files(folder_path, combined_file_path):\n",
    "    combined_data = pd.DataFrame()  # Create an empty DataFrame to hold the combined data\n",
    "\n",
    "    # Iterate through all CSV files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            print('file_path')\n",
    "            # Read the data from the current CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Append the data to the combined DataFrame\n",
    "            combined_data = combined_data.append(df, ignore_index=True)\n",
    "\n",
    "            # Delete the original CSV file\n",
    "            os.remove(file_path)\n",
    "\n",
    "    # Save the combined data to a new CSV file\n",
    "    combined_data.to_csv(combined_file_path, index=False)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Replace with the actual folder path\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\KIIT\\\\Desktop\\\\Capstone_project\\\\Data\\\\Chandigarh'\n",
    "\n",
    "# Replace with the desired combined file path\n",
    "combined_file_path = 'C:\\\\Users\\\\KIIT\\\\Desktop\\\\Capstone_project\\\\Data\\\\Chandigarh\\\\Flats.csv'\n",
    "\n",
    "combine_csv_files(folder_path, combined_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb730050",
   "metadata": {},
   "source": [
    "**Overview**:\n",
    "The function combine_csv_files combines all the CSV files located in a specified folder into a single CSV file. After appending the data from each individual file to the combined file, the original file is deleted.\n",
    "\n",
    "\n",
    "**Function Definition**:\n",
    "\n",
    "_combine_csv_files(folder_path, combined_file_path)_:<br>\n",
    "- _folder_path_: Path to the folder containing the CSV files you want to combine.\n",
    "- _combined_file_path_: Path where the combined CSV file should be saved.\n",
    "\n",
    "**Initialize an Empty DataFrame**:\n",
    "\n",
    "- combined_data = pd.DataFrame(): An empty DataFrame combined_data is created to hold all the data from the individual CSV files.\n",
    "\n",
    "**Iterate Through CSV Files**:\n",
    "\n",
    "- The for loop iterates over each file in the directory specified by folder_path.\n",
    "- Within the loop, the code checks if the current file ends with .csv to ensure that only CSV files are processed.\n",
    "\n",
    "**Read and Append Data**:\n",
    "\n",
    "- file_path = os.path.join(folder_path, file_name): Constructs the full path to the current CSV file.\n",
    "- df = pd.read_csv(file_path): Reads the data from the current CSV file into a DataFrame df.\n",
    "- combined_data = combined_data.append(df, ignore_index=True): Appends the data from df to the combined_data DataFrame. The ignore_index=True parameter ensures that the index is reset and continuous in the combined data.\n",
    "\n",
    "**Delete the Original CSV File**:\n",
    "\n",
    "- os.remove(file_path): Deletes the original CSV file after its data has been appended to the combined data. This step helps in conserving storage space.\n",
    "\n",
    "**Save the Combined Data**:\n",
    "\n",
    "- combined_data.to_csv(combined_file_path, index=False): Writes the combined_data DataFrame to a new CSV file at the specified combined_file_path. The parameter index=False ensures that the DataFrame's index is not written to the CSV.\n",
    "\n",
    "**Example Usage**:\n",
    "\n",
    "- The provided paths (folder_path and combined_file_path) specify the location of the individual CSV files and the path for the combined CSV file, respectively.\n",
    "- Calling the combine_csv_files function with these paths will combine all CSV files in the specified folder and save the combined data to the desired location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(combined_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acb2662b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (173414981.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[29], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    jupyter notebook stop 8888\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53b097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
